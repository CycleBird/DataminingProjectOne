{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8719f70",
   "metadata": {},
   "source": [
    "## PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bffe27",
   "metadata": {},
   "source": [
    "#### Math Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return float(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def r2_variance(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    R^2 = 1 - MSE / Variance(observed)\n",
    "    \"\"\"\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    var_obs = np.var(y_true)\n",
    "    if var_obs == 0:\n",
    "        return 0.0\n",
    "    return float(1.0 - mse(y_true, y_pred) / var_obs)\n",
    "\n",
    "def gradient_descent(X: np.ndarray, y: np.ndarray, lr: float = 0.01, iters: int = 5000, record_every: int = 100, w0: np.ndarray | None = None, b0: float | None = None,) -> tuple[np.ndarray, float, list[float]]:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "\n",
    "    n, p = X.shape\n",
    "    w = np.zeros(p, dtype=float) if w0 is None else np.asarray(w0, dtype=float).reshape(p)\n",
    "    b = 0.0 if b0 is None else float(b0)\n",
    "\n",
    "    losses = []\n",
    "    for t in range(1, iters + 1):\n",
    "        y_hat = X @ w + b\n",
    "        err = y_hat - y\n",
    "\n",
    "        grad_w = (2.0 / n) * (X.T @ err)\n",
    "        grad_b = (2.0 / n) * np.sum(err)\n",
    "\n",
    "        w -= lr * grad_w\n",
    "        b -= lr * grad_b\n",
    "\n",
    "        if record_every and (t % record_every == 0 or t == 1 or t == iters):\n",
    "            losses.append(float(np.mean(err ** 2)))\n",
    "\n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    return X @ w + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21f344",
   "metadata": {},
   "source": [
    "#### Train/Test split + preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_split(csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    target_candidates = [c for c in df.columns if \"compressive strength\" in c.lower()]\n",
    "    target_col = target_candidates[0]\n",
    "\n",
    "    test_df = df.iloc[500:630].copy()\n",
    "    train_df = pd.concat([df.iloc[:500], df.iloc[630:]], axis=0).copy()\n",
    "\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_Xy(df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, list[str], str]:\n",
    "    target_candidates = [c for c in df.columns if \"compressive strength\" in c.lower()]\n",
    "    target_col = target_candidates[0]\n",
    "    feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "    X = df[feature_cols].to_numpy(dtype=float)\n",
    "    y = df[target_col].to_numpy(dtype=float)\n",
    "    return X, y, feature_cols, target_col\n",
    "\n",
    "def Normal_transform(X_train: np.ndarray, X_test: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    X_train = np.asarray(X_train, dtype=float)\n",
    "    X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "    x_min = X_train.min(axis=0)\n",
    "    x_max = X_train.max(axis=0)\n",
    "    denom = (x_max - x_min)\n",
    "    denom_safe = np.where(denom == 0, 1.0, denom)\n",
    "\n",
    "    X_train_mm = (X_train - x_min) / denom_safe\n",
    "    X_test_mm = (X_test - x_min) / denom_safe\n",
    "    return X_train_mm, X_test_mm, x_min, x_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128a20b",
   "metadata": {},
   "source": [
    "#### Experiments and Record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_univariate(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, feature_cols: list[str], scaled_label: str, grid: list[dict], record_every: int = 200,) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    for j in range(n_features):\n",
    "        xtr = X_train[:, [j]]\n",
    "        xte = X_test[:, [j]]\n",
    "\n",
    "        for cfg in grid:\n",
    "            lr = cfg[\"lr\"]\n",
    "            iters = cfg[\"iters\"]\n",
    "\n",
    "            w, b, losses = gradient_descent(xtr, y_train, lr=lr, iters=iters, record_every=record_every)\n",
    "            # print(feature_cols[j], losses[:3], \"...\", losses[-3:])\n",
    "            yhat_tr = predict(xtr, w, b)\n",
    "            yhat_te = predict(xte, w, b)\n",
    "\n",
    "            rows.append({\n",
    "                \"model_type\": \"univariate\",\n",
    "                \"processe\": scaled_label,\n",
    "                \"feature\": feature_cols[j],\n",
    "                \"LR\": lr,\n",
    "                \"iters\": iters,\n",
    "                \"w\": float(w[0]),\n",
    "                \"b\": float(b),\n",
    "                \"train_mse\": mse(y_train, yhat_tr),\n",
    "                \"train_r2\": r2_variance(y_train, yhat_tr),\n",
    "                \"test_mse\": mse(y_test, yhat_te),\n",
    "                \"test_r2\": r2_variance(y_test, yhat_te),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def run_multivariate(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, feature_cols: list[str], scaled_label: str, grid: list[dict], record_every: int = 200,\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for cfg in grid:\n",
    "        lr = cfg[\"lr\"]\n",
    "        iters = cfg[\"iters\"]\n",
    "\n",
    "        w, b, _ = gradient_descent(X_train, y_train, lr=lr, iters=iters, record_every=record_every)\n",
    "        yhat_tr = predict(X_train, w, b)\n",
    "        yhat_te = predict(X_test, w, b)\n",
    "\n",
    "        row = {\n",
    "            \"model_type\": \"multivariate\",\n",
    "            \"processe\": scaled_label,\n",
    "            \"feature\": \"ALL\",\n",
    "            \"LR\": lr,\n",
    "            \"iters\": iters,\n",
    "            \"b\": float(b),\n",
    "            \"train_mse\": mse(y_train, yhat_tr),\n",
    "            \"train_r2\": r2_variance(y_train, yhat_tr),\n",
    "            \"test_mse\": mse(y_test, yhat_te),\n",
    "            \"test_r2\": r2_variance(y_test, yhat_te),\n",
    "        }\n",
    "\n",
    "        for name, coef in zip(feature_cols, w):\n",
    "            row[f\"w_{name}\"] = float(coef)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def select_best_by_train_r2(df: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:\n",
    "    return df.sort_values([\"train_r2\", \"test_r2\"], ascending=False).head(top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacf3b1",
   "metadata": {},
   "source": [
    "#### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2cbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH = \"Concrete_Data.csv\"\n",
    "\n",
    "    # Load & split\n",
    "    train_df, test_df = load_and_split(CSV_PATH)\n",
    "    X_train_raw, y_train, feature_cols, target_col = get_Xy(train_df)\n",
    "    X_test_raw, y_test, _, _ = get_Xy(test_df)\n",
    "\n",
    "    # Build normalized version\n",
    "    X_train_norm, X_test_norm, x_min, x_max = Normal_transform(X_train_raw, X_test_raw)\n",
    "\n",
    "    # Some hyperparameter I tried\n",
    "    GRID_SCALED = [\n",
    "        # {\"lr\": 0.05, \"iters\": 3000},\n",
    "        {\"lr\": 0.01, \"iters\": 5000},\n",
    "        # {\"lr\": 0.005, \"iters\": 8000},\n",
    "    ]\n",
    "\n",
    "    GRID_RAW = [\n",
    "        # {\"lr\": 1e-6,  \"iters\": 5000},\n",
    "        # {\"lr\": 5e-7,  \"iters\": 10000},\n",
    "        {\"lr\": 1e-7,  \"iters\": 30000},\n",
    "        # {\"lr\": 5e-8,  \"iters\": 60000},\n",
    "        # {\"lr\": 1e-8,  \"iters\": 120000},\n",
    "        # {\"lr\": 5e-9,  \"iters\": 200000},\n",
    "    ]\n",
    "\n",
    "    # Univariate\n",
    "    uni_scaled = run_univariate(\n",
    "        X_train_norm, y_train, X_test_norm, y_test, feature_cols,\n",
    "        scaled_label=\"normal\", grid=GRID_SCALED, record_every=200\n",
    "    )\n",
    "    uni_raw = run_univariate(\n",
    "        X_train_raw, y_train, X_test_raw, y_test, feature_cols,\n",
    "        scaled_label=\"raw\", grid=GRID_RAW, record_every=200\n",
    "    )\n",
    "\n",
    "    # Multivariate\n",
    "    multi_scaled = run_multivariate(\n",
    "        X_train_norm, y_train, X_test_norm, y_test, feature_cols,\n",
    "        scaled_label=\"normal\", grid=GRID_SCALED, record_every=200\n",
    "    )\n",
    "    multi_raw = run_multivariate(\n",
    "        X_train_raw, y_train, X_test_raw, y_test, feature_cols,\n",
    "        scaled_label=\"raw\", grid=GRID_RAW, record_every=200\n",
    "    )\n",
    "\n",
    "    # Save ALL results\n",
    "    all_results = pd.concat([uni_scaled, uni_raw, multi_scaled, multi_raw], ignore_index=True)\n",
    "\n",
    "    all_results.to_csv(\"PartB_AllResults.csv\", index=False)\n",
    "    uni_scaled.to_csv(\"PartB_Univariate_Normalized.csv\", index=False)\n",
    "    uni_raw.to_csv(\"PartB_Univariate_Raw.csv\", index=False)\n",
    "    multi_scaled.to_csv(\"PartB_Multivariate_Normalized.csv\", index=False)\n",
    "    multi_raw.to_csv(\"PartB_Multivariate_Raw.csv\", index=False)\n",
    "\n",
    "    print(\"Done Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513293a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train R2 (Coarse): 0.0162696049004859\n",
      "OLS w, b: -0.027384298984679402 63.59263087693311\n"
     ]
    }
   ],
   "source": [
    "\n",
    "j = feature_cols.index(\"Coarse Aggregate  (component 6)(kg in a m^3 mixture)\")\n",
    "x = X_train_raw[:, j]\n",
    "y = y_train\n",
    "\n",
    "X = np.column_stack([np.ones_like(x), x])  # [1, x]\n",
    "b_hat, w_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "yhat = b_hat + w_hat * x\n",
    "print(\"OLS train R2 (Coarse):\", r2_variance(y, yhat))\n",
    "print(\"OLS w, b:\", w_hat, b_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2ea6cf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (OLS) ~ w=-0.0273843, b=63.59263, trainR2=0.01627\n",
      "\n",
      "{'lr': 1e-08, 'iters': 600000} | w= 0.03750573176769714  b= 0.004844533046615009  | train_r2= -0.07566388596658391  test_r2= -0.5384975914024814  | last_loss= 297.97928552400487\n"
     ]
    }
   ],
   "source": [
    "feat = \"Coarse Aggregate  (component 6)(kg in a m^3 mixture)\"\n",
    "j = feature_cols.index(feat)\n",
    "\n",
    "xtr = X_train_raw[:, [j]]\n",
    "xte = X_test_raw[:, [j]]\n",
    "\n",
    "grid = [\n",
    "    # {\"lr\": 1e-8, \"iters\": 100000},\n",
    "    # {\"lr\": 1e-8, \"iters\": 300000},\n",
    "    {\"lr\": 1e-8, \"iters\": 600000},\n",
    "    # {\"lr\": 2e-8, \"iters\": 200000},\n",
    "    # {\"lr\": 5e-8, \"iters\": 150000},\n",
    "]\n",
    "\n",
    "print(\"Target (OLS) ~ w=-0.0273843, b=63.59263, trainR2=0.01627\\n\")\n",
    "\n",
    "for cfg in grid:\n",
    "    w, b, losses = gradient_descent(\n",
    "        xtr, y_train,\n",
    "        lr=cfg[\"lr\"],\n",
    "        iters=cfg[\"iters\"],\n",
    "        record_every=max(cfg[\"iters\"] // 5, 1)  # print a few points\n",
    "    )\n",
    "    yhat_tr = predict(xtr, w, b)\n",
    "    yhat_te = predict(xte, w, b)\n",
    "\n",
    "    tr_r2 = r2_variance(y_train, yhat_tr)\n",
    "    te_r2 = r2_variance(y_test, yhat_te)\n",
    "\n",
    "    print(cfg,\n",
    "          \"| w=\", float(w[0]),\n",
    "          \" b=\", float(b),\n",
    "          \" | train_r2=\", float(tr_r2),\n",
    "          \" test_r2=\", float(te_r2),\n",
    "          \" | last_loss=\", losses[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27219037",
   "metadata": {},
   "source": [
    "#### Q2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5a27b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.1\n",
      "New m1: -4.4\n",
      "New m2: -6.2\n",
      "New m3: -8.0\n",
      "New b : -0.8\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "X = np.array([[3, 4, 5]], dtype=float) # shape (1,3)\n",
    "y = np.array([4], dtype=float)\n",
    "\n",
    "w0 = np.ones(3)\n",
    "b0 = 1.0\n",
    "\n",
    "w1, b1, _ = gradient_descent(X, y, lr=alpha, iters=1, record_every=None, w0=w0, b0=b0)\n",
    "\n",
    "print(\"Q2.1\")\n",
    "print(\"New m1:\", w1[0])\n",
    "print(\"New m2:\", w1[1])\n",
    "print(\"New m3:\", w1[2])\n",
    "print(\"New b :\", b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02427f",
   "metadata": {},
   "source": [
    "#### Q2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c7b3b625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.2\n",
      "New m1: -10.080000000000002\n",
      "New m2: -3.5200000000000005\n",
      "New m3: -4.840000000000001\n",
      "New b : -0.72\n",
      "[-10.08  -3.52  -4.84] -0.72\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "w0 = np.ones(3)\n",
    "b0 = 1.0\n",
    "\n",
    "X = np.array([\n",
    "    [3, 4, 4],\n",
    "    [4, 2, 1],\n",
    "    [10, 2, 5],\n",
    "    [3, 4, 5],\n",
    "    [11, 1, 1],\n",
    "], dtype=float)\n",
    "\n",
    "y = np.array([3, 2, 8, 4, 5], dtype=float)\n",
    "\n",
    "w, b, _ = gradient_descent(X, y, lr=alpha, iters=1, record_every=None, w0=w0, b0=b0)\n",
    "\n",
    "print(\"Q2.2\")\n",
    "print(\"New m1:\", w[0])\n",
    "print(\"New m2:\", w[1])\n",
    "print(\"New m3:\", w[2])\n",
    "print(\"New b :\", b)\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0524d12",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e136800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "CSV_PATH = \"Concrete_Data.csv\" # In case run independently\n",
    "\n",
    "def _add_intercept_df(X: np.ndarray, feature_cols: list[str]) -> pd.DataFrame:\n",
    "    X_df = pd.DataFrame(np.asarray(X, dtype=float), columns=feature_cols)\n",
    "    X_df = sm.add_constant(X_df, has_constant=\"add\")\n",
    "    X_df = X_df.rename(columns={\"const\": \"Intercept\"})\n",
    "    return X_df\n",
    "\n",
    "def fit_ols(X_train: np.ndarray, y_train: np.ndarray, feature_cols: list[str]):\n",
    "    \"\"\"\n",
    "    y = b + Xw\n",
    "    \"\"\"\n",
    "    X_design = _add_intercept_df(X_train, feature_cols)\n",
    "    y = np.asarray(y_train, dtype=float).reshape(-1)\n",
    "    return sm.OLS(y, X_design).fit()\n",
    "\n",
    "def ols_predict(res, X: np.ndarray, feature_cols: list[str]) -> np.ndarray:\n",
    "    X_design = _add_intercept_df(X, feature_cols)\n",
    "    return np.asarray(res.predict(X_design), dtype=float)\n",
    "\n",
    "def extract_pvalues(res, label: str) -> pd.DataFrame:\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"set\": label,\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"p_value\": res.pvalues.values,\n",
    "        \"std_err\": res.bse.values,\n",
    "        \"t_value\": res.tvalues.values,\n",
    "    })\n",
    "\n",
    "def performance_row(label: str, y_train, yhat_train, y_test, yhat_test) -> dict:\n",
    "    return {\n",
    "        \"set\": label,\n",
    "        \"train_mse\": mse(y_train, yhat_train),\n",
    "        \"train_r2\": r2_variance(y_train, yhat_train),\n",
    "        \"test_mse\": mse(y_test, yhat_test),\n",
    "        \"test_r2\": r2_variance(y_test, yhat_test),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e623969",
   "metadata": {},
   "source": [
    "#### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d87545df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done Saved\n"
     ]
    }
   ],
   "source": [
    "# Load & split\n",
    "train_df, test_df = load_and_split(CSV_PATH)\n",
    "X_train_raw, y_train, feature_cols, target_col = get_Xy(train_df)\n",
    "X_test_raw, y_test, _, _ = get_Xy(test_df)\n",
    "\n",
    "X_train_norm, X_test_norm, x_min, x_max = Normal_transform(X_train_raw, X_test_raw)\n",
    "X_train_log = np.log1p(X_train_raw)\n",
    "X_test_log = np.log1p(X_test_raw)\n",
    "\n",
    "sets = {\n",
    "    \"Set2_Raw\": (X_train_raw, X_test_raw),\n",
    "    \"Set1_MinMax01\": (X_train_norm, X_test_norm),\n",
    "    \"Set3_Log1p\": (X_train_log, X_test_log),\n",
    "}\n",
    "\n",
    "# Fit + p-values + performance\n",
    "p_tables = []\n",
    "perf_rows = []\n",
    "\n",
    "for label, (Xtr, Xte) in sets.items():\n",
    "    res = fit_ols(Xtr, y_train, feature_cols)\n",
    "    p_tables.append(extract_pvalues(res, label=label))\n",
    "\n",
    "    yhat_tr = ols_predict(res, Xtr, feature_cols)\n",
    "    yhat_te = ols_predict(res, Xte, feature_cols)\n",
    "    perf_rows.append(performance_row(label, y_train, yhat_tr, y_test, yhat_te))\n",
    "\n",
    "# Combine & Save\n",
    "p_all = pd.concat(p_tables, ignore_index=True)\n",
    "perf_df = pd.DataFrame(perf_rows)\n",
    "\n",
    "p_all.to_csv(\"PartC_PValues_AllSets.csv\", index=False)\n",
    "perf_df.to_csv(\"PartC_Performance_AllSets.csv\", index=False)\n",
    "\n",
    "print(\"\\nDone Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
